{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You are asked to complete the following files:\n",
    "* **pruned_layers.py**, which contains the pruning of DNNs to reduce the storage of insignificant weight parameters with 2 methods: pruning by percentage and prune by standara deviation.\n",
    "* **train_util.py**, which includes the training process of DNNs with pruned connections.\n",
    "* **quantize.py**, which applies the quantization (weight sharing) part on the DNN to reduce the storage of weight parameters.\n",
    "* **huffman_coding.py**, which applies the Huffman coding onto the weight of DNNs to further compress the weight size.\n",
    "\n",
    "You are asked to submit the following files:\n",
    "* **net_before_pruning.pt**, which is the weight parameters before applying pruning on DNN weight parameters.\n",
    "* **net_after_pruning.pt**, which is the weight parameters after applying pruning on DNN weight parameters.\n",
    "* **net_after_quantization.pt**, which is the weight parameters after applying quantization (weight sharing) on DNN weight parameters.\n",
    "* **codebook_vgg16.npy**, which is the quantization codebook of each layer after applying quantization (weight sharing).\n",
    "* **huffman_encoding.npy**, which is the encoding map of each item within the quantization codebook in the whole DNN architecture.\n",
    "* **huffman_freq.npy**, which is the frequency map of each item within the quantization codebook in the whole DNN. \n",
    "\n",
    "To ensure fair grading policy, we fix the choice of model to VGG16_half, which is a down-scaled version of VGG16 using a width multiplier of 0.5. You may check the implementation in **vgg16.py** for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from vgg16 import VGG16, VGG16_half\n",
    "from train_util import train, finetune_after_prune, test, finetune_after_quantization\n",
    "from quantize import quantize_whole_model\n",
    "from huffman_coding import huffman_coding\n",
    "from summary import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "from prune import prune\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "import sklearn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-precision model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = VGG16_half()\n",
    "net = net.to(device)\n",
    "\n",
    "# Uncomment to load pretrained weights\n",
    "#net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "\n",
    "\n",
    "# Comment if you have loaded pretrained weights\n",
    "# Tune the hyperparameters here.\n",
    "INITIAL_LR = 0.055\n",
    "REG = 8e-4\n",
    "EPOCHS = 75 #20\n",
    "BATCH_SIZE = 256\n",
    "train(net, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=INITIAL_LR, reg=REG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3108, Test accuracy=0.9152\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight parameters\n",
    "net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Summary before pruning-----\n",
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t864\t\t\t0.000000\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t9216\t\t9216\t\t\t0.000000\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t18432\t\t18432\t\t\t0.000000\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "10\t\tConvolutional\t36864\t\t36864\t\t\t0.000000\n",
      "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "13\t\tConvolutional\t73728\t\t73728\t\t\t0.000000\n",
      "14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "16\t\tConvolutional\t147456\t\t147456\t\t\t0.000000\n",
      "17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "19\t\tConvolutional\t147456\t\t147456\t\t\t0.000000\n",
      "20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "22\t\tConvolutional\t294912\t\t294912\t\t\t0.000000\n",
      "23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "25\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "28\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tLinear\t\t65536\t\t65536\t\t\t0.000000\n",
      "41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tLinear\t\t65536\t\t65536\t\t\t0.000000\n",
      "44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tLinear\t\t2560\t\t2560\t\t\t0.000000\n",
      "Total nonzero parameters: 3811680\n",
      "Total parameters: 3811680\n",
      "Total sparsity: 0.000000\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Summary before pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning & Finetune with pruned connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy before fine-tuning\n",
    "prune(net, method='std', q=66.8753, s = 1.25) # 1.25\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load pretrained weights\n",
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "finetune_after_prune(net, epochs=10, batch_size=256, lr=0.001, reg=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3480, Test accuracy=0.8995\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight parameters\n",
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Summary After pruning-----\n",
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t113\t\t\t0.869213\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t9216\t\t1107\t\t\t0.879883\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t18432\t\t2856\t\t\t0.845052\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "10\t\tConvolutional\t36864\t\t6190\t\t\t0.832086\n",
      "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "13\t\tConvolutional\t73728\t\t12272\t\t\t0.833550\n",
      "14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "16\t\tConvolutional\t147456\t\t24806\t\t\t0.831774\n",
      "17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "19\t\tConvolutional\t147456\t\t24098\t\t\t0.836575\n",
      "20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "22\t\tConvolutional\t294912\t\t44970\t\t\t0.847514\n",
      "23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "25\t\tConvolutional\t589824\t\t84209\t\t\t0.857230\n",
      "26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "28\t\tConvolutional\t589824\t\t81276\t\t\t0.862203\n",
      "29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tConvolutional\t589824\t\t75491\t\t\t0.872011\n",
      "32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tConvolutional\t589824\t\t72493\t\t\t0.877094\n",
      "35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tConvolutional\t589824\t\t54521\t\t\t0.907564\n",
      "38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tLinear\t\t65536\t\t10625\t\t\t0.837875\n",
      "41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tLinear\t\t65536\t\t8450\t\t\t0.871063\n",
      "44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tLinear\t\t2560\t\t283\t\t\t0.889453\n",
      "Total nonzero parameters: 503760\n",
      "Total parameters: 3811680\n",
      "Total sparsity: 0.867838\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Summary After pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "centers = quantize_whole_model(net, bits=4)\n",
    "\n",
    "# np.save(\"codebook_vgg16.npy\", centers)\n",
    "\n",
    "# print(\"Saving...\")\n",
    "# torch.save(net.state_dict(), \"net_after_quantization.pt\")\n",
    "\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3525, Test accuracy=0.8972\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to load pretrained weights\n",
    "net.load_state_dict(torch.load(\"net_after_quantization.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "#finetune_after_quantization(net, epochs=5, batch_size=256, lr=0.001, reg=5e-5)\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t113\t\t\t0.869213\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t9216\t\t1107\t\t\t0.879883\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t18432\t\t2856\t\t\t0.845052\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "10\t\tConvolutional\t36864\t\t6190\t\t\t0.832086\n",
      "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "13\t\tConvolutional\t73728\t\t12272\t\t\t0.833550\n",
      "14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "16\t\tConvolutional\t147456\t\t24806\t\t\t0.831774\n",
      "17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "19\t\tConvolutional\t147456\t\t24098\t\t\t0.836575\n",
      "20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "22\t\tConvolutional\t294912\t\t44970\t\t\t0.847514\n",
      "23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "25\t\tConvolutional\t589824\t\t84209\t\t\t0.857230\n",
      "26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "28\t\tConvolutional\t589824\t\t81276\t\t\t0.862203\n",
      "29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tConvolutional\t589824\t\t75491\t\t\t0.872011\n",
      "32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tConvolutional\t589824\t\t72493\t\t\t0.877094\n",
      "35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tConvolutional\t589824\t\t54521\t\t\t0.907564\n",
      "38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tLinear\t\t65536\t\t10625\t\t\t0.837875\n",
      "41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tLinear\t\t65536\t\t8450\t\t\t0.871063\n",
      "44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tLinear\t\t2560\t\t283\t\t\t0.889453\n",
      "Total nonzero parameters: 503760\n",
      "Total parameters: 3811680\n",
      "Total sparsity: 0.867838\n"
     ]
    }
   ],
   "source": [
    "summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.7788 bits\n",
      "Complete 1 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.6694 bits\n",
      "Complete 2 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.6401 bits\n",
      "Complete 3 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.6302 bits\n",
      "Complete 4 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.6285 bits\n",
      "Complete 5 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.6752 bits\n",
      "Complete 6 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.6237 bits\n",
      "Complete 7 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.5612 bits\n",
      "Complete 8 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.4531 bits\n",
      "Complete 9 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.4547 bits\n",
      "Complete 10 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.4317 bits\n",
      "Complete 11 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.4503 bits\n",
      "Complete 12 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.4675 bits\n",
      "Complete 13 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.5048 bits\n",
      "Complete 14 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.6289 bits\n",
      "Complete 15 layers for Huffman Coding...\n",
      "Original storage for each parameter: 5.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 4.9011 bits\n",
      "Complete 16 layers for Huffman Coding...\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_after_quantization.pt\"))\n",
    "centers = np.load(\"codebook_vgg16.npy\")\n",
    "frequency_map, encoding_map = huffman_coding(net, centers)\n",
    "np.save(\"huffman_encoding\", encoding_map)\n",
    "np.save(\"huffman_freq\", frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.3798, 0.3615, 0.1040, 0.2815, 0.4390, 0.1532, 0.5035, 0.1199, 0.0064,\n",
      "        0.2706, 0.1908, 0.2142, 0.2110, 0.0194, 0.3515, 0.3837, 0.0053, 0.3496,\n",
      "        0.1642, 0.2042, 0.2816, 0.1916, 0.2265, 0.5013, 0.0033, 0.3221, 0.0905,\n",
      "        0.4349, 0.3615, 0.3752, 0.3417, 0.3994], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #prune(net, method='percentage', q = 50)\n",
    "# #test(net)\n",
    "# #plt.plot(x, norm.pdf(x))\n",
    "# #plt.hist(net.classifer[0].linear.weight.data.cpu()[1], bins='auto')\n",
    "# params = list(net.parameters())\n",
    "# layer1weights = np.array(params[0].data.cpu().numpy()).flatten()\n",
    "# #print(layer1weights)\n",
    "# #print(np.array(params[0].data.cpu()))\n",
    "# #print(np.std(layer1weights))\n",
    "# print(np.percentile(abs(layer1weights), q=5.0))\n",
    "# pruned_data = layer1weights.copy()\n",
    "# pruned_data[abs(pruned_data)<=np.percentile(abs(layer1weights), q=20.0)] = None\n",
    "# plt.hist(abs(layer1weights))\n",
    "# plt.show()\n",
    "# plt.hist(pruned_data, bins =40)\n",
    "\n",
    "# Visualize Weight Distributions\n",
    "params = list(net.parameters())\n",
    "print(params[:][1])\n",
    "layer1weights = np.array(params[0].data.cpu().numpy()).flatten()[np.nonzero(np.array(params[0].data.cpu().numpy()).flatten())]\n",
    "plt.hist(layer1weights, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out how k-means works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = np.array([1,2,3,4,5,6,7,8,9,10]).reshape(-1,1)\n",
    "# kmean = sklearn.cluster.KMeans(n_clusters = 4,#2**bits, \n",
    "#                            init='k-means++', \n",
    "#                            n_init=10, \n",
    "#                            max_iter=300)\n",
    "# labels = kmean.fit_predict(A)\n",
    "# print(labels)\n",
    "# print(kmean.cluster_centers_)\n",
    "# print(labels==3)\n",
    "# new = np.ones(A.shape)\n",
    "# new[labels==3] = kmean.cluster_centers_[3]\n",
    "# print(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out how I can make the huffman encodings/frequency dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter, OrderedDict\n",
    "# A = np.array([0,1,0,2,0,3,0,1,0,4,0,5,0,1,0,2,0,3])\n",
    "# non_zero_A = map(str, A[np.nonzero(A)])\n",
    "# freq = dict(Counter(non_zero_A))\n",
    "# print(freq)\n",
    "# for key, value in freq.items():\n",
    "#     print(key, '->', value)\n",
    "# cp = freq.copy()\n",
    "# print(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_list = []\n",
    "# inter = [1,2,3]\n",
    "# another = [inter,'1', '2', '3']\n",
    "# test_list.extend(another)\n",
    "# print(test_list[0])\n",
    "# inter[0] = 0\n",
    "# print(test_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing if Huffman Encoding is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HuffmanNode():\n",
    "#     def __init__(self, key = '<!$>_ANTHONY_<$!>', freq = 0, right = None, left = None, leaf = False):\n",
    "#         if leaf:\n",
    "#             self.key = key\n",
    "#             self.freq = freq\n",
    "#             self.right = None\n",
    "#             self.left = None\n",
    "#             self.leaf = True\n",
    "#             self.encode = ''\n",
    "#         else:\n",
    "#             self.key = key\n",
    "#             self.freq = right.freq + left.freq\n",
    "#             self.right = right\n",
    "#             self.left = left\n",
    "#             self.leaf = False\n",
    "#             right.add_encode('1')\n",
    "#             left.add_encode('0')\n",
    "#         return\n",
    "#     def add_encode(self, addition):\n",
    "#         if self.leaf == False:\n",
    "#             if self.left == None:\n",
    "#                 self.right.add_encode(addition)\n",
    "#             elif self.right == None:\n",
    "#                 self.left.add_encode(addition)\n",
    "#             else:\n",
    "#                 self.right.add_encode(addition)\n",
    "#                 self.left.add_encode(addition)\n",
    "#             if self.right == None and self.left == None:\n",
    "#                 print('Error: Recursively iterating on a leaf')\n",
    "#         else:\n",
    "#             self.encode = addition + self.encode\n",
    "#         return\n",
    "\n",
    "# def convert_freq_dict_to_encodings(freq):\n",
    "#     original_freq = freq.copy() # Just in Case I want to check something later\n",
    "    \n",
    "#     leaf_list = []\n",
    "#     for centroid, frequency in freq.items():\n",
    "#         leaf_list.append(HuffmanNode(key = centroid,\n",
    "#                                      freq = frequency,\n",
    "#                                      leaf = True))\n",
    "#     tree = []\n",
    "#     tree.extend(leaf_list)\n",
    "    \n",
    "#     MaxIter = 500\n",
    "#     iter = 0\n",
    "#     not_root = True\n",
    "    \n",
    "#     # Forming Huffman Tree and Setting Encoding\n",
    "#     while not_root and iter < MaxIter:\n",
    "#         least_freq_item = tree.pop(-1)\n",
    "#         second_least_freq_item = tree.pop(-1)\n",
    "#         tree.append(HuffmanNode(key = 'Branch ' + str(iter),\n",
    "#                                 right = second_least_freq_item,\n",
    "#                                 left = least_freq_item))\n",
    "#         iter+=1\n",
    "#         not_root = len(tree) > 1\n",
    "#         if not_root:\n",
    "#             if tree[-1].freq > tree[-2].freq:\n",
    "#                 tree = sorted(tree, key=lambda node: node.freq, reverse = True)\n",
    "#     encodings = {}\n",
    "#     for leaf in leaf_list:\n",
    "#         encodings[leaf.key] = leaf.encode\n",
    "#     return encodings\n",
    "\n",
    "\n",
    "# def _huffman_coding_per_layer(weight, centers):\n",
    "#     \"\"\"\n",
    "#     Huffman coding for each layer\n",
    "#     :param weight: weight parameter of the current layer.\n",
    "#     :param centers: KMeans centroids in the quantization codebook of the current weight layer.\n",
    "#     :return: \n",
    "#             'encodings': Encoding map mapping each weight parameter to its Huffman coding.\n",
    "#             'frequency': Frequency map mapping each weight parameter to the total number of its appearance.\n",
    "#             'encodings' should be in this format:\n",
    "#             {\"0.24315\": '0', \"-0.2145\": \"100\", \"1.1234e-5\": \"101\", ...\n",
    "#             }\n",
    "#             'frequency' should be in this format:\n",
    "#             {\"0.25235\": 100, \"-0.2145\": 42, \"1.1234e-5\": 36, ...\n",
    "#             }\n",
    "#             'encodings' and 'frequency' does not need to be ordered in any way.\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     Generate Huffman Coding and Frequency Map according to incoming weights and centers (KMeans centroids).\n",
    "#     --------------Your Code---------------------\n",
    "#     \"\"\"\n",
    "#     non_zero_weights = list(map(str, weight[np.nonzero(weight)]))# creates string array of non-zero weight values\n",
    "#     ordered = Counter(non_zero_weights)\n",
    "#     # creates dictionary of centroids in decending order of frequency\n",
    "#     frequency = {}\n",
    "#     for item in ordered.most_common(len(ordered)):\n",
    "#         key = item[0]\n",
    "#         value = item[1]\n",
    "#         frequency[key] = value\n",
    "#     encodings = convert_freq_dict_to_encodings(frequency) # converts freq dict to centroid encodings\n",
    "#     return encodings, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_matrix = np.array([1,0,1,0,1,2,3,4,5,6,6,0,1,1,1]).reshape(-1,1)\n",
    "# kmean = sklearn.cluster.KMeans(n_clusters = 7, \n",
    "#                                init='k-means++', \n",
    "#                                 n_init=20, \n",
    "#                                max_iter=300)\n",
    "# labels = kmean.fit_predict(weight_matrix)\n",
    "# print(labels)\n",
    "# print(kmean.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings,frequency = _huffman_coding_per_layer(weight_matrix, kmean.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
